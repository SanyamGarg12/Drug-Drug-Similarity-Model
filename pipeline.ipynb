{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install ace_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 04:29:36,511 - INFO - Loading data from cancer_drugs_smiles_from_chembl_pubchem.csv\n",
      "2025-04-20 04:29:36,530 - INFO - Processing 321 molecules\n",
      "2025-04-20 04:29:38,004 - INFO - Skipping drug Afamitresgene Autoleucel - no SMILES provided\n",
      "2025-04-20 04:29:38,124 - INFO - Skipping drug Aflibercept - no SMILES provided\n",
      "2025-04-20 04:29:38,125 - INFO - Skipping drug Aldesleukin - no SMILES provided\n",
      "2025-04-20 04:29:38,318 - INFO - Processed 10 of 321 molecules\n",
      "2025-04-20 04:29:38,319 - INFO - Skipping drug Alemtuzumab - no SMILES provided\n",
      "2025-04-20 04:29:38,547 - INFO - Skipping drug Amivantamab - no SMILES provided\n",
      "2025-04-20 04:29:38,873 - INFO - Processed 20 of 321 molecules\n",
      "[04:29:38] UFFTYPER: Unrecognized charge state for atom: 0\n",
      "[04:29:38] UFFTYPER: Unrecognized atom type: As1+3 (0)\n",
      "[04:29:38] UFFTYPER: Unrecognized charge state for atom: 0\n",
      "[04:29:38] UFFTYPER: Unrecognized atom type: As1+3 (0)\n",
      "[04:29:38] UFFTYPER: Unrecognized charge state for atom: 0\n",
      "[04:29:38] UFFTYPER: Unrecognized atom type: As1+3 (0)\n",
      "[04:29:38] UFFTYPER: Unrecognized charge state for atom: 1\n",
      "[04:29:38] UFFTYPER: Unrecognized atom type: As1+3 (1)\n",
      "2025-04-20 04:29:38,962 - INFO - Skipping drug Asparaginase - no SMILES provided\n",
      "2025-04-20 04:29:38,963 - INFO - Skipping drug Atezolizumab - no SMILES provided\n",
      "2025-04-20 04:29:39,223 - INFO - Skipping drug Avelumab - no SMILES provided\n",
      "2025-04-20 04:29:39,224 - INFO - Skipping drug Axicabtagene Ciloleucel - no SMILES provided\n",
      "2025-04-20 04:29:39,315 - INFO - Skipping drug BCG Vaccine - no SMILES provided\n",
      "2025-04-20 04:29:39,315 - INFO - Skipping drug Belantamab Mafodotin - no SMILES provided\n",
      "2025-04-20 04:29:39,578 - INFO - Skipping drug Bevacizumab - no SMILES provided\n",
      "[04:29:39] UFFTYPER: Unrecognized charge state for atom: 54\n",
      "2025-04-20 04:29:41,439 - WARNING - Failed to embed molecule\n",
      "2025-04-20 04:29:41,440 - WARNING - Failed to generate 3D structure for Bleomycin\n",
      "2025-04-20 04:29:41,441 - INFO - Skipping drug Blinatumomab - no SMILES provided\n",
      "2025-04-20 04:29:41,691 - INFO - Skipping drug Brentuximab vedotin - no SMILES provided\n",
      "2025-04-20 04:29:41,692 - INFO - Skipping drug Brexucabtagene Autoleucel - no SMILES provided\n",
      "2025-04-20 04:29:45,481 - INFO - Processed 50 of 321 molecules\n",
      "2025-04-20 04:29:45,482 - INFO - Skipping drug Calaspargase Pegol - no SMILES provided\n",
      "[04:29:45] UFFTYPER: Unrecognized atom type: Pt3+2 (0)\n",
      "[04:29:45] UFFTYPER: Unrecognized atom type: Pt3+2 (10)\n",
      "2025-04-20 04:29:46,537 - INFO - Skipping drug Cemiplimab - no SMILES provided\n",
      "2025-04-20 04:29:46,734 - INFO - Processed 60 of 321 molecules\n",
      "2025-04-20 04:29:46,735 - INFO - Skipping drug Cetuximab - no SMILES provided\n",
      "2025-04-20 04:29:46,773 - INFO - Skipping drug Ciltacabtagene Autoleucel - no SMILES provided\n",
      "[04:29:46] UFFTYPER: Unrecognized atom type: Pt6+2 (1)\n",
      "[04:29:46] UFFTYPER: Unrecognized atom type: Pt6+2 (1)\n",
      "2025-04-20 04:29:47,206 - INFO - Processed 70 of 321 molecules\n",
      "2025-04-20 04:29:50,742 - INFO - Skipping drug Daratumumab - no SMILES provided\n",
      "2025-04-20 04:29:50,743 - INFO - Skipping drug Daratumumab, Hyaluronidase - no SMILES provided\n",
      "2025-04-20 04:29:51,117 - INFO - Processed 80 of 321 molecules\n",
      "2025-04-20 04:29:51,118 - INFO - Skipping drug Daunorubicin, Cytarabine Liposome - no SMILES provided\n",
      "2025-04-20 04:29:51,140 - INFO - Skipping drug Decitabine , Cedazuridine - no SMILES provided\n",
      "2025-04-20 04:29:53,681 - WARNING - Failed to embed molecule\n",
      "2025-04-20 04:29:53,682 - WARNING - Failed to generate 3D structure for Degarelix\n",
      "2025-04-20 04:29:53,682 - INFO - Skipping drug Denileukin Diftitox - no SMILES provided\n",
      "2025-04-20 04:29:53,683 - INFO - Skipping drug Denosumab - no SMILES provided\n",
      "2025-04-20 04:29:53,866 - INFO - Skipping drug Dinutuximab - no SMILES provided\n",
      "2025-04-20 04:29:55,533 - INFO - Processed 90 of 321 molecules\n",
      "2025-04-20 04:29:55,534 - INFO - Skipping drug Dostarlimab - no SMILES provided\n",
      "2025-04-20 04:29:55,935 - INFO - Skipping drug Durvalumab - no SMILES provided\n",
      "2025-04-20 04:29:58,809 - INFO - Skipping drug Elotuzumab - no SMILES provided\n",
      "2025-04-20 04:29:58,810 - INFO - Skipping drug Elranatamab - no SMILES provided\n",
      "2025-04-20 04:29:58,811 - INFO - Skipping drug Emapalumab - no SMILES provided\n",
      "2025-04-20 04:29:59,007 - INFO - Skipping drug Enfortumab Vedotin - no SMILES provided\n",
      "2025-04-20 04:29:59,294 - INFO - Skipping drug Epcoritamab - no SMILES provided\n",
      "2025-04-20 04:30:00,276 - INFO - Processed 110 of 321 molecules\n",
      "2025-04-20 04:30:02,617 - INFO - Processed 120 of 321 molecules\n",
      "[04:30:02] UFFTYPER: Unrecognized charge state for atom: 23\n",
      "[04:30:03] UFFTYPER: Unrecognized charge state for atom: 23\n",
      "2025-04-20 04:30:03,515 - INFO - Skipping drug Gemtuzumab Ozogamicin - no SMILES provided\n",
      "2025-04-20 04:30:03,834 - INFO - Skipping drug Glofitamab - no SMILES provided\n",
      "2025-04-20 04:30:08,309 - WARNING - Failed to embed molecule\n",
      "2025-04-20 04:30:08,310 - WARNING - Failed to generate 3D structure for Histrelin\n",
      "2025-04-20 04:30:08,314 - INFO - Skipping drug Ibritumomab Tiuxetan - no SMILES provided\n",
      "2025-04-20 04:30:08,631 - INFO - Skipping drug Idecabtagene Vicleucel - no SMILES provided\n",
      "2025-04-20 04:30:08,748 - INFO - Processed 140 of 321 molecules\n",
      "2025-04-20 04:30:09,182 - INFO - Skipping drug Inotuzumab Ozogamicin - no SMILES provided\n",
      "2025-04-20 04:30:09,246 - INFO - Skipping drug Ipilimumab - no SMILES provided\n",
      "2025-04-20 04:30:09,986 - INFO - Processed 150 of 321 molecules\n",
      "2025-04-20 04:30:09,987 - INFO - Skipping drug Isatuximab - no SMILES provided\n",
      "2025-04-20 04:30:12,505 - INFO - Processed 160 of 321 molecules\n",
      "2025-04-20 04:30:14,374 - INFO - Skipping drug Lifileucel - no SMILES provided\n",
      "2025-04-20 04:30:14,375 - INFO - Skipping drug Lisocabtagene Maraleucel - no SMILES provided\n",
      "2025-04-20 04:30:14,402 - INFO - Skipping drug Loncastuximab Tesirine - no SMILES provided\n",
      "2025-04-20 04:30:14,558 - INFO - Skipping drug Lu 177-Dotatate - no SMILES provided\n",
      "2025-04-20 04:30:15,040 - INFO - Skipping drug Lutetium Lu 177 Vipivotide Tetraxetan - no SMILES provided\n",
      "2025-04-20 04:30:15,041 - INFO - Skipping drug Margetuximab - no SMILES provided\n",
      "2025-04-20 04:30:18,120 - WARNING - Failed to embed molecule\n",
      "2025-04-20 04:30:18,121 - WARNING - Failed to generate 3D structure for Mifamurtide\n",
      "2025-04-20 04:30:19,641 - INFO - Skipping drug Mogamulizumab - no SMILES provided\n",
      "2025-04-20 04:30:19,642 - INFO - Skipping drug Mosunetuzumab - no SMILES provided\n",
      "2025-04-20 04:30:19,643 - INFO - Skipping drug Moxetumomab Pasudotox - no SMILES provided\n",
      "2025-04-20 04:30:23,175 - INFO - Skipping drug Nadofaragene Firadenovec - no SMILES provided\n",
      "2025-04-20 04:30:23,176 - INFO - Skipping drug Naxitamab - no SMILES provided\n",
      "2025-04-20 04:30:23,176 - INFO - Skipping drug Necitumumab - no SMILES provided\n",
      "2025-04-20 04:30:24,164 - INFO - Skipping drug Nivolumab - no SMILES provided\n",
      "2025-04-20 04:30:24,165 - INFO - Skipping drug Obinutuzumab - no SMILES provided\n",
      "2025-04-20 04:30:24,165 - INFO - Skipping drug Odronextamab - no SMILES provided\n",
      "2025-04-20 04:30:24,165 - INFO - Skipping drug Ofatumumab - no SMILES provided\n",
      "2025-04-20 04:30:24,278 - INFO - Skipping drug Olaratumab - no SMILES provided\n",
      "[04:30:24] UFFTYPER: Unrecognized atom type: Pt3+2 (0)\n",
      "[04:30:24] UFFTYPER: Unrecognized atom type: Pt3+2 (8)\n",
      "2025-04-20 04:30:30,934 - INFO - Processed 210 of 321 molecules\n",
      "[04:30:31] UFFTYPER: Unrecognized atom type: Pd3+2 (0)\n",
      "[04:30:31] UFFTYPER: Unrecognized atom type: Pd3+2 (52)\n",
      "2025-04-20 04:30:31,527 - INFO - Skipping drug Panitumumab - no SMILES provided\n",
      "2025-04-20 04:30:31,680 - INFO - Skipping drug Pegaspargase - no SMILES provided\n",
      "2025-04-20 04:30:31,681 - INFO - Skipping drug Peginterferon Alfa-2b - no SMILES provided\n",
      "2025-04-20 04:30:31,682 - INFO - Skipping drug Pembrolizumab - no SMILES provided\n",
      "2025-04-20 04:30:31,941 - INFO - Processed 220 of 321 molecules\n",
      "2025-04-20 04:30:31,982 - INFO - Skipping drug Pertuzumab - no SMILES provided\n",
      "2025-04-20 04:30:31,983 - INFO - Skipping drug Pertuzumab, Trastuzumab, Hyaluronidase - no SMILES provided\n",
      "2025-04-20 04:30:32,485 - INFO - Skipping drug Polatuzumab Vedotin - no SMILES provided\n",
      "2025-04-20 04:30:32,486 - INFO - Skipping drug Polyestradiol Phosphate - no SMILES provided\n",
      "2025-04-20 04:30:32,514 - INFO - Processed 230 of 321 molecules\n",
      "2025-04-20 04:30:32,681 - INFO - Skipping drug Porfimer - no SMILES provided\n",
      "2025-04-20 04:30:33,509 - INFO - Skipping drug Racotumomab - no SMILES provided\n",
      "[04:30:33] UFFTYPER: Unrecognized atom type: Ra1 (1)\n",
      "[04:30:33] UFFTYPER: Unrecognized atom type: Ra1 (1)\n",
      "2025-04-20 04:30:33,512 - INFO - Processed 240 of 321 molecules\n",
      "2025-04-20 04:30:33,628 - INFO - Skipping drug Ramucirumab - no SMILES provided\n",
      "2025-04-20 04:30:33,678 - INFO - Skipping drug Realgar-Indigo Naturalis Formulation - no SMILES provided\n",
      "2025-04-20 04:30:33,753 - INFO - Skipping drug Relatlimab ,  Nivolumab - no SMILES provided\n",
      "2025-04-20 04:30:33,995 - INFO - Skipping drug Retifanlimab - no SMILES provided\n",
      "2025-04-20 04:30:34,113 - INFO - Processed 250 of 321 molecules\n",
      "2025-04-20 04:30:34,219 - INFO - Skipping drug Rituximab - no SMILES provided\n",
      "2025-04-20 04:30:34,219 - INFO - Skipping drug Rituximab, Hyaluronidase - no SMILES provided\n",
      "2025-04-20 04:30:38,412 - WARNING - Failed to embed molecule\n",
      "2025-04-20 04:30:38,413 - WARNING - Failed to generate 3D structure for Sacituzumab Govitecan\n",
      "2025-04-20 04:30:38,657 - INFO - Skipping drug Sipuleucel-T - no SMILES provided\n",
      "2025-04-20 04:30:39,198 - INFO - Skipping drug Tafasitamab - no SMILES provided\n",
      "2025-04-20 04:30:39,199 - INFO - Skipping drug Tagraxofusp - no SMILES provided\n",
      "2025-04-20 04:30:39,273 - INFO - Skipping drug Talimogene Laherparepvec - no SMILES provided\n",
      "2025-04-20 04:30:39,274 - INFO - Skipping drug Talquetamab - no SMILES provided\n",
      "2025-04-20 04:30:39,386 - INFO - Skipping drug Tarlatamab - no SMILES provided\n",
      "2025-04-20 04:30:39,631 - INFO - Skipping drug Tebentafusp - no SMILES provided\n",
      "2025-04-20 04:30:39,632 - INFO - Skipping drug Teclistamab - no SMILES provided\n",
      "2025-04-20 04:30:39,942 - INFO - Processed 280 of 321 molecules\n",
      "2025-04-20 04:30:42,586 - INFO - Skipping drug Tisagenlecleucel - no SMILES provided\n",
      "2025-04-20 04:30:42,586 - INFO - Skipping drug Tislelizumab - no SMILES provided\n",
      "2025-04-20 04:30:42,587 - INFO - Skipping drug Tisotumab Vedotin - no SMILES provided\n",
      "2025-04-20 04:30:42,806 - INFO - Processed 290 of 321 molecules\n",
      "2025-04-20 04:30:42,907 - INFO - Skipping drug Toripalimab - no SMILES provided\n",
      "2025-04-20 04:30:44,058 - INFO - Skipping drug Trastuzumab - no SMILES provided\n",
      "2025-04-20 04:30:44,059 - INFO - Skipping drug Trastuzumab Deruxtecan - no SMILES provided\n",
      "2025-04-20 04:30:45,078 - INFO - Skipping drug Trastuzumab, Hyaluronidase - no SMILES provided\n",
      "2025-04-20 04:30:45,079 - INFO - Skipping drug Tremelimumab - no SMILES provided\n",
      "2025-04-20 04:30:46,936 - WARNING - Failed to embed molecule\n",
      "2025-04-20 04:30:46,937 - WARNING - Failed to generate 3D structure for Triptorelin\n",
      "2025-04-20 04:30:48,429 - INFO - Processed 310 of 321 molecules\n",
      "2025-04-20 04:30:53,758 - INFO - Processed 320 of 321 molecules\n",
      "2025-04-20 04:30:53,759 - INFO - Skipping drug Zolbetuximab - no SMILES provided\n",
      "2025-04-20 04:30:53,759 - INFO - Successfully processed 225 valid molecules out of 321\n",
      "2025-04-20 04:30:53,759 - INFO - Calculating descriptors...\n",
      "100%|██████████| 225/225 [00:51<00:00,  4.37it/s]\n",
      "2025-04-20 04:31:45,634 - INFO - Saving results to cancer_smiles_descriptors.csv\n",
      "2025-04-20 04:31:46,109 - INFO - Successfully saved 225 drugs with descriptors to cancer_smiles_descriptors.csv\n",
      "2025-04-20 04:31:46,110 - INFO - Descriptor calculation summary:\n",
      "2025-04-20 04:31:46,110 - INFO -   Total drugs in input: 321\n",
      "2025-04-20 04:31:46,112 - INFO -   Drugs with valid SMILES and 3D structures: 225\n",
      "2025-04-20 04:31:46,116 - INFO -   Number of descriptors calculated: 1826\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, MolFromSmiles\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mordred import Calculator, descriptors\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def t3d(mol):\n",
    "    \"\"\"\n",
    "    Convert a molecule to a 3D structure.\n",
    "    Returns None if conversion fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if mol is None:\n",
    "            return None  # Return None for invalid molecules\n",
    "\n",
    "        mol_3D = Chem.AddHs(mol)\n",
    "\n",
    "        # Embed the 3D structure\n",
    "        if AllChem.EmbedMolecule(mol_3D, maxAttempts=5000) != 0:\n",
    "            logger.warning(\"Failed to embed molecule\")\n",
    "            return None  # Embedding failed\n",
    "\n",
    "        AllChem.UFFOptimizeMolecule(mol_3D)\n",
    "\n",
    "        # Compute Gasteiger charges\n",
    "        AllChem.ComputeGasteigerCharges(mol_3D)\n",
    "\n",
    "        return mol_3D\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error processing molecule: {e}')\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Input and output file paths\n",
    "    input_csv = 'cancer_drugs_smiles_from_chembl_pubchem.csv'\n",
    "    output_csv = 'cancer_smiles_descriptors.csv'\n",
    "    \n",
    "    logger.info(f\"Loading data from {input_csv}\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_csv):\n",
    "        logger.error(f\"Input file {input_csv} not found\")\n",
    "        return\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Initialize salt remover\n",
    "    remover = SaltRemover()\n",
    "    \n",
    "    # Create lists to store results\n",
    "    valid_mols = []\n",
    "    drug_names = []\n",
    "    smiles_strings = []\n",
    "    chembl_ids = []\n",
    "    \n",
    "    # Count total molecules\n",
    "    total_mols = len(df)\n",
    "    logger.info(f\"Processing {total_mols} molecules\")\n",
    "    \n",
    "    # Process each molecule\n",
    "    for i, row in df.iterrows():\n",
    "        drug_name = row.get('Drug Name', '')\n",
    "        smile = row.get('SMILES', '')\n",
    "        chembl_id = row.get('ChEMBL ID', '')\n",
    "        \n",
    "        # Skip rows with no SMILES\n",
    "        if pd.isna(smile) or smile == '':\n",
    "            logger.info(f\"Skipping drug {drug_name} - no SMILES provided\")\n",
    "            continue\n",
    "            \n",
    "        # Convert SMILES to molecule\n",
    "        mol = MolFromSmiles(smile)\n",
    "        if mol:\n",
    "            # Strip salts\n",
    "            stripped_mol = remover.StripMol(mol)\n",
    "            # Convert to 3D\n",
    "            mol_3D = t3d(stripped_mol)\n",
    "            if mol_3D:\n",
    "                valid_mols.append(mol_3D)\n",
    "                drug_names.append(drug_name)\n",
    "                smiles_strings.append(smile)\n",
    "                chembl_ids.append(chembl_id)\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    logger.info(f\"Processed {i+1} of {total_mols} molecules\")\n",
    "            else:\n",
    "                logger.warning(f\"Failed to generate 3D structure for {drug_name}\")\n",
    "        else:\n",
    "            logger.warning(f\"Failed to parse SMILES for {drug_name}: {smile}\")\n",
    "    \n",
    "    logger.info(f\"Successfully processed {len(valid_mols)} valid molecules out of {total_mols}\")\n",
    "    \n",
    "    # Calculate descriptors\n",
    "    logger.info(\"Calculating descriptors...\")\n",
    "    try:\n",
    "        calc = Calculator(descriptors, ignore_3D=False)\n",
    "        descriptor_df = calc.pandas(valid_mols)\n",
    "        \n",
    "        # Add drug information to descriptor dataframe\n",
    "        descriptor_df['Drug Name'] = drug_names\n",
    "        descriptor_df['SMILES'] = smiles_strings\n",
    "        descriptor_df['ChEMBL ID'] = chembl_ids\n",
    "        \n",
    "        # Reorder columns to have drug info first\n",
    "        cols = descriptor_df.columns.tolist()\n",
    "        info_cols = ['Drug Name', 'SMILES', 'ChEMBL ID']\n",
    "        descriptor_cols = [col for col in cols if col not in info_cols]\n",
    "        descriptor_df = descriptor_df[info_cols + descriptor_cols]\n",
    "        \n",
    "        # Save to CSV\n",
    "        logger.info(f\"Saving results to {output_csv}\")\n",
    "        descriptor_df.to_csv(output_csv, index=False)\n",
    "        logger.info(f\"Successfully saved {len(descriptor_df)} drugs with descriptors to {output_csv}\")\n",
    "        \n",
    "        # Print a summary\n",
    "        logger.info(f\"Descriptor calculation summary:\")\n",
    "        logger.info(f\"  Total drugs in input: {total_mols}\")\n",
    "        logger.info(f\"  Drugs with valid SMILES and 3D structures: {len(valid_mols)}\")\n",
    "        logger.info(f\"  Number of descriptors calculated: {len(descriptor_cols)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating descriptors: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiley_biodata_csv_path = \"cancer_smiles_descriptors.csv\"\n",
    "output_csv = \"cancer_filtered_biodata_data.csv\"\n",
    "cross_mapped_csv = \"cancer_final_data.csv\"\n",
    "\n",
    "STR_allowed_threshold = 0.20\n",
    "replace_String_with = 'median'\n",
    "Smiles_column_name = 'SMILES'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "neuro_smiley_csv = \"cancer_drugs_target_smiles.csv\"\n",
    "neuro_smile_col = \"SMILES\"\n",
    "neuro_target_col = \"Targets\"\n",
    "neuro_drug_name_col = \"Product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping 185 columns out of 1087 string columns out of 1829 total columns with more than 20.0% NaN values\n",
      "Filled 902 columns with median values\n"
     ]
    }
   ],
   "source": [
    "def improove_raw_data(raw_file_path, output_csv_path):\n",
    "    smiley_biodata_data = pd.read_csv(raw_file_path)\n",
    "    smiley_biodata_data[Smiles_column_name] = smiley_biodata_data[Smiles_column_name].str.lstrip('\\n')\n",
    "    smiley_biodata_data = smiley_biodata_data.drop_duplicates(subset=Smiles_column_name, keep='first')\n",
    "    cols = smiley_biodata_data.columns\n",
    "    val_counts = {}\n",
    "    for col in cols:\n",
    "        val_counts[col] = smiley_biodata_data[col].apply(lambda x: type(x)).value_counts()\n",
    "    str_cols = []\n",
    "    for key in val_counts.keys():\n",
    "        if str in val_counts[key].keys():\n",
    "            str_cols.append(key)\n",
    "    if Smiles_column_name in str_cols:\n",
    "        str_cols.remove(Smiles_column_name)\n",
    "        \n",
    "        \n",
    "    for str_col in str_cols:\n",
    "        initial_Data = smiley_biodata_data[str_col]\n",
    "        final_Data = []\n",
    "        for i in initial_Data:\n",
    "            n = np.nan\n",
    "            try:\n",
    "                n = float(i)\n",
    "            except:\n",
    "                n = np.nan\n",
    "            final_Data.append(n)\n",
    "        smiley_biodata_data[str_col] = final_Data\n",
    "    \n",
    "    nan_cols = smiley_biodata_data.columns[smiley_biodata_data.isna().any()].tolist()\n",
    "    nan_counts = {}\n",
    "    for nan_col in nan_cols:\n",
    "        n = smiley_biodata_data[nan_col].isna().sum()\n",
    "        if n in nan_counts.keys():\n",
    "            nan_counts[n].append(nan_col)\n",
    "        else:\n",
    "            nan_counts[n] = [nan_col]\n",
    "    # sort nan_counts\n",
    "    nan_counts = dict(sorted(nan_counts.items()))\n",
    "    n_data = smiley_biodata_data.shape[0]\n",
    "    str_allowed = int(n_data * STR_allowed_threshold)\n",
    "    str_cols_to_drop = []\n",
    "    for x,y in nan_counts.items():\n",
    "        if x > str_allowed:\n",
    "            for col in y:\n",
    "                str_cols_to_drop.append(col)\n",
    "                \n",
    "                \n",
    "    print(f\"Droping {len(str_cols_to_drop)} columns out of {len(str_cols)} string columns out of {len(cols)} total columns with more than {STR_allowed_threshold*100}% NaN values\")\n",
    "    smiley_biodata_data = smiley_biodata_data.drop(columns=str_cols_to_drop)\n",
    "    \n",
    "    count = 0\n",
    "    for col in smiley_biodata_data.columns:\n",
    "        if smiley_biodata_data[col].isna().sum() > 0:\n",
    "            count += 1\n",
    "            # print(col)\n",
    "            median = smiley_biodata_data[col].median()\n",
    "            smiley_biodata_data[col] = smiley_biodata_data[col].fillna(median)\n",
    "            \n",
    "    print(f\"Filled {count} columns with median values\")\n",
    "    smiley_biodata_data.to_csv(output_csv, index=False)\n",
    "    \n",
    "\n",
    "improove_raw_data(smiley_biodata_csv_path, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product', 'Targets', 'SMILES'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "neuro_smiley_data = pd.read_csv(neuro_smiley_csv)\n",
    "filtered_smiley_data = pd.read_csv(output_csv)\n",
    "neuro_cols = list(neuro_smiley_data.columns)\n",
    "print(neuro_cols, type(neuro_cols))\n",
    "neuro_cols.remove(neuro_smile_col)\n",
    "neuro_cols.remove(neuro_drug_name_col)\n",
    "# print(neuro_target_col)\n",
    "neuro_cols.remove(neuro_target_col)\n",
    "neuro_smiley_data = neuro_smiley_data.drop(columns=neuro_cols)\n",
    "# neuro_smiley_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstrip \\n from smile column\n",
    "neuro_smiley_data[neuro_smile_col] = neuro_smiley_data[neuro_smile_col].str.lstrip('\\n')\n",
    "\n",
    "# upper case all drugs and targets\n",
    "neuro_smiley_data[neuro_drug_name_col] = neuro_smiley_data[neuro_drug_name_col].str.upper()\n",
    "# filtered_smiley_data\n",
    "neuro_smiley_data[neuro_target_col] = neuro_smiley_data[neuro_target_col].str.upper()\n",
    "# neuro_smiley_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Targets</th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>CDKN2A</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>PGR</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>CDK4</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>ERBB2</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>CDK6</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>VORINOSTAT</td>\n",
       "      <td>TUBA4A</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccccc1)NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>TP53</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>BTK</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>EFNB2</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>MYD88</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Product  Targets                                             SMILES\n",
       "0     ABEMACICLIB   CDKN2A  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB      PGR  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB     CDK4  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB    ERBB2  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB     CDK6  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "..            ...      ...                                                ...\n",
       "229    VORINOSTAT   TUBA4A                        O=C(CCCCCCC(=O)Nc1ccccc1)NO\n",
       "230  ZANUBRUTINIB     TP53  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "230  ZANUBRUTINIB      BTK  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "230  ZANUBRUTINIB    EFNB2  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "230  ZANUBRUTINIB    MYD88  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "\n",
       "[5556 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split targets with \",\", if 2 values in target are sep by \";\", then split them into 2 rows\n",
    "neuro_smiley_data[neuro_target_col] = neuro_smiley_data[neuro_target_col].str.split(';')\n",
    "neuro_smiley_data = neuro_smiley_data.explode(neuro_target_col)\n",
    "neuro_smiley_data\n",
    "# neuro_smiley_data = neuro_smiley_data.dropna()\n",
    "# drop 'nan' strings\n",
    "\n",
    "# neuro_smiley_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1589"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neuro_smiley_data[\"Targets\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Targets</th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>CDKN2A</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>PGR</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>CDK4</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>ERBB2</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>CDK6</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>VORINOSTAT</td>\n",
       "      <td>TUBA4A</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccccc1)NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>TP53</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>BTK</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>EFNB2</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ZANUBRUTINIB</td>\n",
       "      <td>MYD88</td>\n",
       "      <td>C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Product  Targets                                             SMILES\n",
       "0     ABEMACICLIB   CDKN2A  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB      PGR  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB     CDK4  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB    ERBB2  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "0     ABEMACICLIB     CDK6  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...\n",
       "..            ...      ...                                                ...\n",
       "229    VORINOSTAT   TUBA4A                        O=C(CCCCCCC(=O)Nc1ccccc1)NO\n",
       "230  ZANUBRUTINIB     TP53  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "230  ZANUBRUTINIB      BTK  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "230  ZANUBRUTINIB    EFNB2  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "230  ZANUBRUTINIB    MYD88  C=CC(=O)N1CCC([C@@H]2CCNc3c(C(N)=O)c(-c4ccc(Oc...\n",
       "\n",
       "[5556 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuro_smiley_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross map the rows of filtered_smiley_data on the column Smiles_column_name, and for each and create a new df with new col names smiley_1, smiley_2, anf for all other columns, column name should be (lets say it is col_name) col_name_1, col_name_2 an col_name_1 should have the value of the row of filtered_smiley_data where the Smiles_column_name matched with smiley_1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import nan\n",
    "\n",
    "# Load CSV files\n",
    "neuro_smiley_data = pd.read_csv(neuro_smiley_csv)\n",
    "filtered_smiley_data = pd.read_csv(output_csv)\n",
    "\n",
    "# Keep only necessary columns\n",
    "neuro_cols = list(neuro_smiley_data.columns)\n",
    "neuro_cols.remove(neuro_smile_col)\n",
    "neuro_cols.remove(neuro_drug_name_col)\n",
    "neuro_cols.remove(neuro_target_col)\n",
    "neuro_smiley_data = neuro_smiley_data.drop(columns=neuro_cols)\n",
    "\n",
    "# Clean and format text columns\n",
    "neuro_smiley_data[neuro_smile_col] = neuro_smiley_data[neuro_smile_col].str.lstrip('\\n')\n",
    "neuro_smiley_data[neuro_drug_name_col] = neuro_smiley_data[neuro_drug_name_col].str.upper()\n",
    "neuro_smiley_data[neuro_target_col] = neuro_smiley_data[neuro_target_col].str.upper()\n",
    "\n",
    "# Explode targets for each smiley\n",
    "neuro_smiley_data[neuro_target_col] = neuro_smiley_data[neuro_target_col].str.split(';')\n",
    "neuro_smiley_data = neuro_smiley_data.explode(neuro_target_col)\n",
    "\n",
    "# Extract all column names except Smiles_column_name\n",
    "other_columns = [col for col in filtered_smiley_data.columns if col != Smiles_column_name]\n",
    "\n",
    "# Create mappings for fast lookup\n",
    "smiley_to_drug = neuro_smiley_data.set_index(neuro_smile_col)[neuro_drug_name_col].to_dict()\n",
    "smiley_to_targets = neuro_smiley_data.groupby(neuro_smile_col)[neuro_target_col].apply(set).to_dict()\n",
    "\n",
    "# Store filtered_smiley_data rows as a dictionary for fast lookup\n",
    "smiley_to_row = filtered_smiley_data.set_index(Smiles_column_name).to_dict(orient=\"index\")\n",
    "\n",
    "# Get all unique smiley structures\n",
    "smiley_list = filtered_smiley_data[Smiles_column_name].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_pairs = []\n",
    "total_smileys = len(smiley_list)\n",
    "q1 = total_smileys // 4  # Quarter 1 limit\n",
    "\n",
    "# Generate first quarter of pairs\n",
    "for i in range(q1):\n",
    "    for j in range(i + 1, total_smileys):\n",
    "        cross_pairs.append((smiley_list[i], smiley_list[j]))\n",
    "\n",
    "# Process the first quarter\n",
    "new_data = []\n",
    "\n",
    "for smiley_1, smiley_2 in cross_pairs:\n",
    "    row_1 = smiley_to_row[smiley_1]\n",
    "    row_2 = smiley_to_row[smiley_2]\n",
    "\n",
    "    new_row = {}\n",
    "\n",
    "    drug_1 = smiley_to_drug.get(smiley_1, \"UNKNOWN\")\n",
    "    drug_2 = smiley_to_drug.get(smiley_2, \"UNKNOWN\")\n",
    "\n",
    "    drug_1, drug_2 = sorted([drug_1, drug_2])\n",
    "\n",
    "    new_row[\"drug_1\"] = drug_1\n",
    "    new_row[\"drug_2\"] = drug_2\n",
    "\n",
    "    targets1 = smiley_to_targets.get(smiley_1, set())\n",
    "    targets2 = smiley_to_targets.get(smiley_2, set())\n",
    "\n",
    "    common_targets = targets1.intersection(targets2)\n",
    "    common_targets.discard(nan)\n",
    "\n",
    "    new_row[\"matched\"] = bool(common_targets)\n",
    "    new_row[\"common_targets\"] = common_targets\n",
    "    new_row[\"smiley_1\"] = smiley_1\n",
    "    new_row[\"smiley_2\"] = smiley_2    \n",
    "\n",
    "    for col in other_columns:\n",
    "        new_row[f\"{col}_1\"] = row_1[col]\n",
    "        new_row[f\"{col}_2\"] = row_2[col]\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "cross_mapped_df = pd.DataFrame(new_data)\n",
    "cross_mapped_df.to_csv(\"cross_mapped_part1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "common_targets\n",
       "{}                                               6967\n",
       "{ TP53}                                           424\n",
       "{ CYP3A4}                                         139\n",
       "{ FLT3}                                           106\n",
       "{ ERBB2}                                          101\n",
       "                                                 ... \n",
       "{ HRAS,  JAK2}                                      1\n",
       "{ NRAS,  KIT,  JAK2}                                1\n",
       "{ KRAS,  PTEN,  PIK3CA,  NRAS,  HRAS}               1\n",
       "{ KRAS,  PTEN,  PIK3CA,  NRAS,  MAP2K1,  ATM}       1\n",
       "{ APC,  NF1,  PTEN,  PIK3CA,  NRAS,  HRAS}          1\n",
       "Name: count, Length: 1498, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value count common targets\n",
    "common_target_counts = cross_mapped_df[\"common_targets\"].value_counts()\n",
    "common_target_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_pairs = []\n",
    "\n",
    "# Generate second quarter of pairs\n",
    "for i in range(q1, 2 * q1):\n",
    "    for j in range(i + 1, total_smileys):\n",
    "        cross_pairs.append((smiley_list[i], smiley_list[j]))\n",
    "\n",
    "# Process the second quarter\n",
    "new_data = []\n",
    "\n",
    "for smiley_1, smiley_2 in cross_pairs:\n",
    "    row_1 = smiley_to_row[smiley_1]\n",
    "    row_2 = smiley_to_row[smiley_2]\n",
    "\n",
    "    new_row = {}\n",
    "\n",
    "    drug_1 = smiley_to_drug.get(smiley_1, \"UNKNOWN\")\n",
    "    drug_2 = smiley_to_drug.get(smiley_2, \"UNKNOWN\")\n",
    "\n",
    "    drug_1, drug_2 = sorted([drug_1, drug_2])\n",
    "\n",
    "    new_row[\"drug_1\"] = drug_1\n",
    "    new_row[\"drug_2\"] = drug_2\n",
    "\n",
    "    targets1 = smiley_to_targets.get(smiley_1, set())\n",
    "    targets2 = smiley_to_targets.get(smiley_2, set())\n",
    "\n",
    "    common_targets = targets1.intersection(targets2)\n",
    "    common_targets.discard(nan)\n",
    "\n",
    "    new_row[\"matched\"] = bool(common_targets)\n",
    "    new_row[\"common_targets\"] = common_targets\n",
    "    new_row[\"smiley_1\"] = smiley_1\n",
    "    new_row[\"smiley_2\"] = smiley_2    \n",
    "\n",
    "    for col in other_columns:\n",
    "        new_row[f\"{col}_1\"] = row_1[col]\n",
    "        new_row[f\"{col}_2\"] = row_2[col]\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "cross_mapped_df = pd.DataFrame(new_data)\n",
    "cross_mapped_df.to_csv(\"cross_mapped_part2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_pairs = []\n",
    "\n",
    "# Generate third quarter of pairs\n",
    "for i in range(2 * q1, 3 * q1):\n",
    "    for j in range(i + 1, total_smileys):\n",
    "        cross_pairs.append((smiley_list[i], smiley_list[j]))\n",
    "\n",
    "# Process the third quarter\n",
    "new_data = []\n",
    "\n",
    "for smiley_1, smiley_2 in cross_pairs:\n",
    "    row_1 = smiley_to_row[smiley_1]\n",
    "    row_2 = smiley_to_row[smiley_2]\n",
    "\n",
    "    new_row = {}\n",
    "\n",
    "    drug_1 = smiley_to_drug.get(smiley_1, \"UNKNOWN\")\n",
    "    drug_2 = smiley_to_drug.get(smiley_2, \"UNKNOWN\")\n",
    "\n",
    "    drug_1, drug_2 = sorted([drug_1, drug_2])\n",
    "\n",
    "    new_row[\"drug_1\"] = drug_1\n",
    "    new_row[\"drug_2\"] = drug_2\n",
    "\n",
    "    targets1 = smiley_to_targets.get(smiley_1, set())\n",
    "    targets2 = smiley_to_targets.get(smiley_2, set())\n",
    "\n",
    "    common_targets = targets1.intersection(targets2)\n",
    "    common_targets.discard(nan)\n",
    "\n",
    "    new_row[\"matched\"] = bool(common_targets)\n",
    "    new_row[\"common_targets\"] = common_targets\n",
    "    new_row[\"smiley_1\"] = smiley_1\n",
    "    new_row[\"smiley_2\"] = smiley_2    \n",
    "\n",
    "    for col in other_columns:\n",
    "        new_row[f\"{col}_1\"] = row_1[col]\n",
    "        new_row[f\"{col}_2\"] = row_2[col]\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "cross_mapped_df = pd.DataFrame(new_data)\n",
    "cross_mapped_df.to_csv(\"cross_mapped_part3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_pairs = []\n",
    "\n",
    "# Generate fourth quarter of pairs\n",
    "for i in range(3 * q1, total_smileys):\n",
    "    for j in range(i + 1, total_smileys):\n",
    "        cross_pairs.append((smiley_list[i], smiley_list[j]))\n",
    "\n",
    "# Process the fourth quarter\n",
    "new_data = []\n",
    "\n",
    "for smiley_1, smiley_2 in cross_pairs:\n",
    "    row_1 = smiley_to_row[smiley_1]\n",
    "    row_2 = smiley_to_row[smiley_2]\n",
    "\n",
    "    new_row = {}\n",
    "\n",
    "    drug_1 = smiley_to_drug.get(smiley_1, \"UNKNOWN\")\n",
    "    drug_2 = smiley_to_drug.get(smiley_2, \"UNKNOWN\")\n",
    "\n",
    "    drug_1, drug_2 = sorted([drug_1, drug_2])\n",
    "\n",
    "    new_row[\"drug_1\"] = drug_1\n",
    "    new_row[\"drug_2\"] = drug_2\n",
    "\n",
    "    targets1 = smiley_to_targets.get(smiley_1, set())\n",
    "    targets2 = smiley_to_targets.get(smiley_2, set())\n",
    "\n",
    "    common_targets = targets1.intersection(targets2)\n",
    "    common_targets.discard(nan)\n",
    "\n",
    "    new_row[\"matched\"] = bool(common_targets)\n",
    "    new_row[\"common_targets\"] = common_targets\n",
    "    new_row[\"smiley_1\"] = smiley_1\n",
    "    new_row[\"smiley_2\"] = smiley_2    \n",
    "\n",
    "    for col in other_columns:\n",
    "        new_row[f\"{col}_1\"] = row_1[col]\n",
    "        new_row[f\"{col}_2\"] = row_2[col]\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "cross_mapped_df = pd.DataFrame(new_data)\n",
    "cross_mapped_df.to_csv(\"cross_mapped_part4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge all parts\n",
    "df1 = pd.read_csv(\"cross_mapped_part1.csv\")\n",
    "df2 = pd.read_csv(\"cross_mapped_part2.csv\")\n",
    "df3 = pd.read_csv(\"cross_mapped_part3.csv\")\n",
    "df4 = pd.read_csv(\"cross_mapped_part4.csv\")\n",
    "\n",
    "final_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "final_df.to_csv(cross_mapped_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matched\n",
       "False    15797\n",
       "True      8513\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"matched\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matched\n",
       "False    15797\n",
       "True      8513\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"matched\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Tanimoto Morgan: 100%|██████████| 24310/24310 [00:05<00:00, 4844.56it/s] \n",
      "Computing Tanimoto FeatMorgan:   0%|          | 0/24310 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m fp_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMorgan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatMorgan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAtomPair\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRDKit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTorsion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACCS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m fp_types:\n\u001b[0;32m---> 92\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTanimoto_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_tanimoto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Compute MCS Features\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# df[\"MCS_Size\"], df[\"MCS_Tanimoto\"], df[\"MCS_Overlap\"] = compute_mcs_features()\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Save updated dataframe\u001b[39;00m\n\u001b[1;32m     98\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_mapped_data_with_tanimoto_cancer.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 62\u001b[0m, in \u001b[0;36mcompute_tanimoto\u001b[0;34m(fp_type)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sequential computation of Tanimoto similarity.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 62\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mComputing Tanimoto \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfp_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtanimoto_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmiley_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmiley_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:1553\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1551\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[0;32m-> 1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m):\n\u001b[1;32m   1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m klass(v, index\u001b[38;5;241m=\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mk)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[1;32m  12594\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[1;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1737\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m   1736\u001b[0m         result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m-> 1737\u001b[0m         itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem.rdFingerprintGenerator import (\n",
    "    GetMorganGenerator, GetRDKitFPGenerator, GetTopologicalTorsionGenerator, GetAtomPairGenerator\n",
    ")\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.rdmolops import RDKFingerprint\n",
    "from rdkit.Chem import rdFMCS\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cancer_final_data.csv\")\n",
    "\n",
    "# Ensure missing values are handled\n",
    "df = df.dropna(subset=[\"smiley_1\", \"smiley_2\"])  # Drop rows with missing SMILES\n",
    "\n",
    "# Fingerprint cache\n",
    "fingerprint_cache = {}\n",
    "\n",
    "def get_fingerprint(smiles, fp_type):\n",
    "    \"\"\"Generate molecular fingerprints based on type with caching.\"\"\"\n",
    "    if (smiles, fp_type) in fingerprint_cache:\n",
    "        return fingerprint_cache[(smiles, fp_type)]\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    if fp_type == 'Morgan':\n",
    "        fp = GetMorganGenerator(radius=2, fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'FeatMorgan':\n",
    "        fp = GetMorganGenerator(radius=2, fpSize=2048, includeChirality=True).GetFingerprint(mol)\n",
    "    elif fp_type == 'AtomPair':\n",
    "        fp = GetAtomPairGenerator(fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'RDKit':\n",
    "        fp = GetRDKitFPGenerator(fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'Torsion':\n",
    "        fp = GetTopologicalTorsionGenerator(fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'Layered':\n",
    "        fp = RDKFingerprint(mol)\n",
    "    elif fp_type == 'MACCS':\n",
    "        fp = rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown fingerprint type: {fp_type}\")\n",
    "    \n",
    "    fingerprint_cache[(smiles, fp_type)] = fp\n",
    "    return fp\n",
    "\n",
    "def tanimoto_similarity(smiles1, smiles2, fp_type):\n",
    "    \"\"\"Compute Tanimoto similarity between two molecules.\"\"\"\n",
    "    fp1 = get_fingerprint(smiles1, fp_type)\n",
    "    fp2 = get_fingerprint(smiles2, fp_type)\n",
    "\n",
    "    if fp1 is None or fp2 is None:\n",
    "        return None\n",
    "    return DataStructs.FingerprintSimilarity(fp1, fp2)\n",
    "\n",
    "def compute_tanimoto(fp_type):\n",
    "    \"\"\"Sequential computation of Tanimoto similarity.\"\"\"\n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Computing Tanimoto {fp_type}\"):\n",
    "        results.append(tanimoto_similarity(row[\"smiley_1\"], row[\"smiley_2\"], fp_type))\n",
    "    return results\n",
    "\n",
    "def compute_mcs(smiles1, smiles2):\n",
    "    \"\"\"Compute Maximum Common Substructure (MCS) similarity features.\"\"\"\n",
    "    mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "    if mol1 is None or mol2 is None:\n",
    "        return None, None, None\n",
    "\n",
    "    mcs_result = rdFMCS.FindMCS([mol1, mol2])\n",
    "    mcs_size = mcs_result.numAtoms\n",
    "    mcs_tanimoto = mcs_size / min(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "    mcs_overlap = mcs_size / max(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "\n",
    "    return mcs_size, mcs_tanimoto, mcs_overlap\n",
    "\n",
    "def compute_mcs_features():\n",
    "    \"\"\"Sequential computation of MCS features.\"\"\"\n",
    "    mcs_sizes, mcs_tanimotos, mcs_overlaps = [], [], []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing MCS Features\"):\n",
    "        mcs_size, mcs_tanimoto, mcs_overlap = compute_mcs(row[\"smiley_1\"], row[\"smiley_2\"])\n",
    "        mcs_sizes.append(mcs_size)\n",
    "        mcs_tanimotos.append(mcs_tanimoto)\n",
    "        mcs_overlaps.append(mcs_overlap)\n",
    "    return mcs_sizes, mcs_tanimotos, mcs_overlaps\n",
    "\n",
    "# Compute Tanimoto similarities for all fingerprint types\n",
    "fp_types = ['Morgan', 'FeatMorgan', 'AtomPair', 'RDKit', 'Torsion', 'Layered', 'MACCS']\n",
    "for fp in fp_types:\n",
    "    df[f\"Tanimoto_{fp}\"] = compute_tanimoto(fp)\n",
    "\n",
    "# Compute MCS Features\n",
    "# df[\"MCS_Size\"], df[\"MCS_Tanimoto\"], df[\"MCS_Overlap\"] = compute_mcs_features()\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv(\"cross_mapped_data_with_tanimoto_cancer.csv\", index=False)\n",
    "print(\"Feature extraction complete!\")\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_selection import mutual_info_classif, RFE, SelectFromModel, SelectKBest, f_classif\n",
    "# from sklearn.linear_model import LogisticRegression, Lasso\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv(\"cross_mapped_data_with_tanimoto_paper.csv\")  # Updated filename\n",
    "\n",
    "# # Extract feature pairs\n",
    "# feature_pairs = {}\n",
    "# for col in df.columns:\n",
    "#     if \"_\" in col and col not in [\"smiley_1\", \"smiley_2\", \"drug_1\", \"drug_2\", \"matched\"]:\n",
    "#         base = \"_\".join(col.split(\"_\")[:-1])  # Generalized base feature extraction\n",
    "#         if base not in feature_pairs:\n",
    "#             feature_pairs[base] = []\n",
    "#         feature_pairs[base].append(col)\n",
    "\n",
    "# # Ensure pairs are correctly grouped\n",
    "# paired_features = [pair for pair in feature_pairs.values() if len(pair) == 2]\n",
    "# flattened_features = [feat for pair in paired_features for feat in pair]\n",
    "\n",
    "# # Add new Tanimoto and MCS features\n",
    "# additional_features = [\n",
    "#     \"Tanimoto_Morgan\", \"Tanimoto_FeatMorgan\", \"Tanimoto_AtomPair\",\n",
    "#     \"Tanimoto_RDKit\", \"Tanimoto_Torsion\", \"Tanimoto_Layered\", \"Tanimoto_MACCS\",\n",
    "#     \"MCS_Size\", \"MCS_Tanimoto\", \"MCS_Overlap\"\n",
    "# ]\n",
    "# flattened_features.extend(additional_features)\n",
    "\n",
    "# # Define X and y\n",
    "# X = df[flattened_features]\n",
    "# y = df[\"matched\"]\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ### Feature Selection Methods\n",
    "# # 1. Mutual Information\n",
    "# mi_scores = mutual_info_classif(X_train, y_train)\n",
    "# mi_scores_dict = {feat: mi for feat, mi in zip(flattened_features, mi_scores)}\n",
    "# selected_mi = sorted(mi_scores_dict, key=mi_scores_dict.get, reverse=True)[:20]\n",
    "\n",
    "# # 2. RFE with Logistic Regression\n",
    "# log_reg = LogisticRegression(max_iter=1000)\n",
    "# rfe = RFE(log_reg, n_features_to_select=20)\n",
    "# rfe.fit(X_train, y_train)\n",
    "# selected_rfe = [feat for feat, keep in zip(flattened_features, rfe.support_) if keep]\n",
    "\n",
    "# # 3. Random Forest Feature Importance\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# rf_importances = {feat: imp for feat, imp in zip(flattened_features, rf.feature_importances_)}\n",
    "# selected_rf = sorted(rf_importances, key=rf_importances.get, reverse=True)[:20]\n",
    "\n",
    "# # 4. Lasso Regression (L1 Regularization)\n",
    "# lasso = Lasso(alpha=0.01)\n",
    "# lasso.fit(X_train, y_train)\n",
    "# lasso_coeffs = {feat: coef for feat, coef in zip(flattened_features, lasso.coef_)}\n",
    "# selected_lasso = [feat for feat, coef in lasso_coeffs.items() if abs(coef) > 0]\n",
    "\n",
    "# # 5. XGBoost Feature Importance\n",
    "# xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "# xgb.fit(X_train, y_train)\n",
    "# xgb_importances = {feat: imp for feat, imp in zip(flattened_features, xgb.feature_importances_)}\n",
    "# selected_xgb = sorted(xgb_importances, key=xgb_importances.get, reverse=True)[:20]\n",
    "\n",
    "# # 6. SelectKBest (ANOVA F-score)\n",
    "# kbest = SelectKBest(score_func=f_classif, k=20)\n",
    "# kbest.fit(X_train, y_train)\n",
    "# selected_kbest = [feat for feat, keep in zip(flattened_features, kbest.get_support()) if keep]\n",
    "\n",
    "# ### Ensure features are kept in pairs\n",
    "# def enforce_feature_pairs(selected_features):\n",
    "#     selected_pairs = []\n",
    "#     for pair in paired_features:\n",
    "#         if any(f in selected_features for f in pair):\n",
    "#             selected_pairs.extend(pair)\n",
    "#     return list(set(selected_pairs + [f for f in selected_features if f in additional_features]))\n",
    "\n",
    "# selected_mi_pairs = enforce_feature_pairs(selected_mi)\n",
    "# selected_rfe_pairs = enforce_feature_pairs(selected_rfe)\n",
    "# selected_rf_pairs = enforce_feature_pairs(selected_rf)\n",
    "# selected_lasso_pairs = enforce_feature_pairs(selected_lasso)\n",
    "# selected_xgb_pairs = enforce_feature_pairs(selected_xgb)\n",
    "# selected_kbest_pairs = enforce_feature_pairs(selected_kbest)\n",
    "\n",
    "# ### Train Models with Selected Features\n",
    "# def train_model(X_train, X_test, y_train, y_test, features, model):\n",
    "#     X_train_selected = X_train[features]\n",
    "#     X_test_selected = X_test[features]\n",
    "#     model.fit(X_train_selected, y_train)\n",
    "#     y_pred = model.predict(X_test_selected)\n",
    "#     y_pred_proba = model.predict_proba(X_test_selected)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "#     return accuracy_score(y_test, y_pred), roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "#     \"SVM\": SVC(probability=True),  # Enable probability for AUC calculation\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "#     \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "# }\n",
    "\n",
    "# accuracy_results = {}\n",
    "# auc_results = {}\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     accuracy_results[name] = {}\n",
    "#     auc_results[name] = {}\n",
    "#     for method, selected_features in {\n",
    "#         \"Mutual Information\": selected_mi_pairs,\n",
    "#         \"RFE\": selected_rfe_pairs,\n",
    "#         \"Random Forest\": selected_rf_pairs,\n",
    "#         \"Lasso\": selected_lasso_pairs,\n",
    "#         \"XGBoost\": selected_xgb_pairs,\n",
    "#         \"SelectKBest\": selected_kbest_pairs,\n",
    "#     }.items():\n",
    "#         acc, auc = train_model(X_train, X_test, y_train, y_test, selected_features, model)\n",
    "#         accuracy_results[name][method] = acc\n",
    "#         auc_results[name][method] = auc\n",
    "\n",
    "# ### Print Results\n",
    "# print(\"\\nFeature Selection Results (Top Selected Features in Pairs):\")\n",
    "# print(\"Mutual Information:\", selected_mi_pairs)\n",
    "# print(\"RFE:\", selected_rfe_pairs)\n",
    "# print(\"Random Forest:\", selected_rf_pairs)\n",
    "# print(\"Lasso:\", selected_lasso_pairs)\n",
    "# print(\"XGBoost:\", selected_xgb_pairs)\n",
    "# print(\"SelectKBest:\", selected_kbest_pairs)\n",
    "\n",
    "# print(\"\\nModel Performance with Selected Features:\")\n",
    "# for model in models.keys():\n",
    "#     print(f\"\\n{model}:\")\n",
    "#     for method in accuracy_results[model].keys():\n",
    "#         print(f\"  {method}: Accuracy = {accuracy_results[model][method]:.4f}, AUC = {auc_results[model][method]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_1</th>\n",
       "      <th>drug_2</th>\n",
       "      <th>matched</th>\n",
       "      <th>common_targets</th>\n",
       "      <th>smiley_1</th>\n",
       "      <th>smiley_2</th>\n",
       "      <th>ABC_1</th>\n",
       "      <th>ABC_2</th>\n",
       "      <th>ABCGG_1</th>\n",
       "      <th>ABCGG_2</th>\n",
       "      <th>...</th>\n",
       "      <th>mZagreb1_2</th>\n",
       "      <th>mZagreb2_1</th>\n",
       "      <th>mZagreb2_2</th>\n",
       "      <th>Tanimoto_Morgan</th>\n",
       "      <th>Tanimoto_FeatMorgan</th>\n",
       "      <th>Tanimoto_AtomPair</th>\n",
       "      <th>Tanimoto_RDKit</th>\n",
       "      <th>Tanimoto_Torsion</th>\n",
       "      <th>Tanimoto_Layered</th>\n",
       "      <th>Tanimoto_MACCS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>ABIRATERONE</td>\n",
       "      <td>False</td>\n",
       "      <td>set()</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "      <td>C[C@]12CC[C@H](O)CC1=CC[C@@H]1[C@@H]2CC[C@]2(C...</td>\n",
       "      <td>29.255246</td>\n",
       "      <td>21.272672</td>\n",
       "      <td>20.627212</td>\n",
       "      <td>16.066691</td>\n",
       "      <td>...</td>\n",
       "      <td>7.402778</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.250441</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.309859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>ACALABRUTINIB</td>\n",
       "      <td>False</td>\n",
       "      <td>set()</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "      <td>CC#CC(=O)N1CCC[C@H]1c1nc(-c2ccc(C(=O)Nc3ccccn3...</td>\n",
       "      <td>29.255246</td>\n",
       "      <td>27.541373</td>\n",
       "      <td>20.627212</td>\n",
       "      <td>20.914869</td>\n",
       "      <td>...</td>\n",
       "      <td>10.083333</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.382911</td>\n",
       "      <td>0.523977</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.523977</td>\n",
       "      <td>0.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>ACLARUBICIN</td>\n",
       "      <td>False</td>\n",
       "      <td>set()</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "      <td>CC[C@@]1(O)C[C@H](O[C@H]2C[C@H](N(C)C)[C@H](O[...</td>\n",
       "      <td>29.255246</td>\n",
       "      <td>45.896458</td>\n",
       "      <td>20.627212</td>\n",
       "      <td>32.886584</td>\n",
       "      <td>...</td>\n",
       "      <td>22.090278</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>12.444444</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.289040</td>\n",
       "      <td>0.529257</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.529257</td>\n",
       "      <td>0.395062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>ADAGRASIB</td>\n",
       "      <td>False</td>\n",
       "      <td>set()</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "      <td>C=C(F)C(=O)N1CCN(c2nc(OC[C@@H]3CCCN3C)nc3c2CCN...</td>\n",
       "      <td>29.255246</td>\n",
       "      <td>34.043233</td>\n",
       "      <td>20.627212</td>\n",
       "      <td>24.666546</td>\n",
       "      <td>...</td>\n",
       "      <td>13.027778</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>9.388889</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>0.343342</td>\n",
       "      <td>0.513245</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>0.513245</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEMACICLIB</td>\n",
       "      <td>AFATINIB</td>\n",
       "      <td>False</td>\n",
       "      <td>set()</td>\n",
       "      <td>CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...</td>\n",
       "      <td>CN(C)C/C=C/C(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2...</td>\n",
       "      <td>29.255246</td>\n",
       "      <td>26.548139</td>\n",
       "      <td>20.627212</td>\n",
       "      <td>20.094967</td>\n",
       "      <td>...</td>\n",
       "      <td>10.722222</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>7.444444</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.403279</td>\n",
       "      <td>0.441804</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.441804</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        drug_1         drug_2  matched common_targets  \\\n",
       "0  ABEMACICLIB    ABIRATERONE    False          set()   \n",
       "1  ABEMACICLIB  ACALABRUTINIB    False          set()   \n",
       "2  ABEMACICLIB    ACLARUBICIN    False          set()   \n",
       "3  ABEMACICLIB      ADAGRASIB    False          set()   \n",
       "4  ABEMACICLIB       AFATINIB    False          set()   \n",
       "\n",
       "                                            smiley_1  \\\n",
       "0  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...   \n",
       "1  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...   \n",
       "2  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...   \n",
       "3  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...   \n",
       "4  CCN1CCN(Cc2ccc(Nc3ncc(F)c(-c4cc(F)c5nc(C)n(C(C...   \n",
       "\n",
       "                                            smiley_2      ABC_1      ABC_2  \\\n",
       "0  C[C@]12CC[C@H](O)CC1=CC[C@@H]1[C@@H]2CC[C@]2(C...  29.255246  21.272672   \n",
       "1  CC#CC(=O)N1CCC[C@H]1c1nc(-c2ccc(C(=O)Nc3ccccn3...  29.255246  27.541373   \n",
       "2  CC[C@@]1(O)C[C@H](O[C@H]2C[C@H](N(C)C)[C@H](O[...  29.255246  45.896458   \n",
       "3  C=C(F)C(=O)N1CCN(c2nc(OC[C@@H]3CCCN3C)nc3c2CCN...  29.255246  34.043233   \n",
       "4  CN(C)C/C=C/C(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2...  29.255246  26.548139   \n",
       "\n",
       "     ABCGG_1    ABCGG_2  ...  mZagreb1_2  mZagreb2_1  mZagreb2_2  \\\n",
       "0  20.627212  16.066691  ...    7.402778    8.027778    5.416667   \n",
       "1  20.627212  20.914869  ...   10.083333    8.027778    7.750000   \n",
       "2  20.627212  32.886584  ...   22.090278    8.027778   12.444444   \n",
       "3  20.627212  24.666546  ...   13.027778    8.027778    9.388889   \n",
       "4  20.627212  20.094967  ...   10.722222    8.027778    7.444444   \n",
       "\n",
       "   Tanimoto_Morgan  Tanimoto_FeatMorgan  Tanimoto_AtomPair  Tanimoto_RDKit  \\\n",
       "0         0.089109             0.085714           0.250441        0.299145   \n",
       "1         0.120690             0.120690           0.382911        0.523977   \n",
       "2         0.079710             0.082759           0.289040        0.529257   \n",
       "3         0.097744             0.097744           0.343342        0.513245   \n",
       "4         0.132231             0.141667           0.403279        0.441804   \n",
       "\n",
       "   Tanimoto_Torsion  Tanimoto_Layered  Tanimoto_MACCS  \n",
       "0          0.055118          0.299145        0.309859  \n",
       "1          0.196581          0.523977        0.575758  \n",
       "2          0.076923          0.529257        0.395062  \n",
       "3          0.152672          0.513245        0.636364  \n",
       "4          0.145455          0.441804        0.543210  \n",
       "\n",
       "[5 rows x 3299 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(\"paper_final_data.csv\", )\n",
    "df = pd.read_csv(\"paper_final_data.csv\", usecols=[\"smiley_1\", \"smiley_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979300\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed: 979300\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for _, row in df.iterrows():\n",
    "    count += 1\n",
    "\n",
    "print(\"Rows processed:\", count)  # Should match len(df) = 979300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCS Computation:   0%|          | 0/1959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCS Computation:   3%|▎         | 54/1959 [06:24<3:46:10,  7.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 117\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mcs_sizes, mcs_tanimotos, mcs_overlaps\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Compute Tanimoto similarities in parallel\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# fp_types = ['Morgan', 'FeatMorgan', 'AtomPair', 'RDKit', 'Torsion', 'Layered', 'MACCS']\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# for fp in fp_types:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#     df[f\"Tanimoto_{fp}\"] = parallel_tanimoto(fp)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Compute MCS Features in parallel\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCS_Size\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCS_Tanimoto\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCS_Overlap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_mcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Save updated dataframe\u001b[39;00m\n\u001b[1;32m    120\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiley_mcs_paper.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 100\u001b[0m, in \u001b[0;36mparallel_mcs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), CHUNK_SIZE), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCS Computation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(i \u001b[38;5;241m+\u001b[39m CHUNK_SIZE, \u001b[38;5;28mlen\u001b[39m(df))  \u001b[38;5;66;03m# Ensure last chunk is handled correctly\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CORES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_mcs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msmiles1_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmiles2_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     sizes, tanimotos, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch_results)\n\u001b[1;32m    105\u001b[0m     mcs_sizes[i:end_idx] \u001b[38;5;241m=\u001b[39m sizes\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.rdFingerprintGenerator import (\n",
    "    GetMorganGenerator, GetRDKitFPGenerator, GetTopologicalTorsionGenerator, GetAtomPairGenerator\n",
    ")\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem.rdmolops import RDKFingerprint\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "\n",
    "# Convert SMILES columns to NumPy arrays (faster access)\n",
    "smiles1_arr = df[\"smiley_1\"].to_numpy()\n",
    "smiles2_arr = df[\"smiley_2\"].to_numpy()\n",
    "\n",
    "# Constants\n",
    "NUM_CORES = 7  # Adjust based on available CPU cores\n",
    "CHUNK_SIZE = 500  # Batch processing size\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_fingerprint(smiles, fp_type):\n",
    "    \"\"\"Generate molecular fingerprints with caching.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    if fp_type == 'Morgan':\n",
    "        return GetMorganGenerator(radius=2, fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'FeatMorgan':\n",
    "        return GetMorganGenerator(radius=2, fpSize=2048, includeChirality=True).GetFingerprint(mol)\n",
    "    elif fp_type == 'AtomPair':\n",
    "        return GetAtomPairGenerator(fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'RDKit':\n",
    "        return GetRDKitFPGenerator(fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'Torsion':\n",
    "        return GetTopologicalTorsionGenerator(fpSize=2048).GetFingerprint(mol)\n",
    "    elif fp_type == 'Layered':\n",
    "        return RDKFingerprint(mol)\n",
    "    elif fp_type == 'MACCS':\n",
    "        return rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "    else:\n",
    "        return None  # Unknown fingerprint type\n",
    "\n",
    "def compute_tanimoto(s1, s2, fp_type):\n",
    "    \"\"\"Compute Tanimoto similarity between two molecules.\"\"\"\n",
    "    fp1 = get_fingerprint(s1, fp_type)\n",
    "    fp2 = get_fingerprint(s2, fp_type)\n",
    "\n",
    "    if fp1 is None or fp2 is None:\n",
    "        return 0.0  # Return 0 similarity if fingerprints are invalid\n",
    "    \n",
    "    return DataStructs.FingerprintSimilarity(fp1, fp2)\n",
    "\n",
    "def parallel_tanimoto(fp_type):\n",
    "    \"\"\"Parallel computation of Tanimoto similarity using batch processing.\"\"\"\n",
    "    results = np.zeros(len(df), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(0, len(df), CHUNK_SIZE), desc=f\"Tanimoto {fp_type}\"):\n",
    "        end_idx = min(i + CHUNK_SIZE, len(df))  # Ensure last chunk is handled correctly\n",
    "\n",
    "        batch_results = Parallel(n_jobs=NUM_CORES, backend=\"multiprocessing\")(\n",
    "            delayed(compute_tanimoto)(s1, s2, fp_type)\n",
    "            for s1, s2 in zip(smiles1_arr[i:end_idx], smiles2_arr[i:end_idx])\n",
    "        )\n",
    "\n",
    "        results[i:end_idx] = batch_results\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_mcs(smiles1, smiles2):\n",
    "    \"\"\"Compute Maximum Common Substructure (MCS) similarity features.\"\"\"\n",
    "    mol1, mol2 = Chem.MolFromSmiles(smiles1), Chem.MolFromSmiles(smiles2)\n",
    "    if mol1 is None or mol2 is None:\n",
    "        return 0, 0.0, 0.0\n",
    "\n",
    "    num_atoms = min(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
    "    timeout = 0.5 if num_atoms < 20 else (1.5 if num_atoms < 50 else 2.0)\n",
    "\n",
    "    try:\n",
    "        mcs_result = rdFMCS.FindMCS(mols=[mol1, mol2], maximizeBonds=True, timeout=int(timeout * 1000))\n",
    "        mcs_size = mcs_result.numAtoms\n",
    "        mcs_tanimoto = mcs_size / min(mol1.GetNumAtoms(), mol2.GetNumAtoms()) if mcs_size else 0.0\n",
    "        mcs_overlap = mcs_size / max(mol1.GetNumAtoms(), mol2.GetNumAtoms()) if mcs_size else 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing MCS for {smiles1} and {smiles2}: {e}\")\n",
    "        return 0, 0.0, 0.0\n",
    "\n",
    "    return mcs_size, mcs_tanimoto, mcs_overlap\n",
    "\n",
    "def parallel_mcs():\n",
    "    \"\"\"Parallel computation of MCS features using batch processing.\"\"\"\n",
    "    mcs_sizes = np.zeros(len(df), dtype=np.float32)\n",
    "    mcs_tanimotos = np.zeros(len(df), dtype=np.float32)\n",
    "    mcs_overlaps = np.zeros(len(df), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(0, len(df), CHUNK_SIZE), desc=\"MCS Computation\"):\n",
    "        end_idx = min(i + CHUNK_SIZE, len(df))  # Ensure last chunk is handled correctly\n",
    "\n",
    "        batch_results = Parallel(n_jobs=NUM_CORES)(\n",
    "            delayed(compute_mcs)(s1, s2) for s1, s2 in zip(smiles1_arr[i:end_idx], smiles2_arr[i:end_idx])\n",
    "        )\n",
    "        sizes, tanimotos, overlaps = zip(*batch_results)\n",
    "\n",
    "        mcs_sizes[i:end_idx] = sizes\n",
    "        mcs_tanimotos[i:end_idx] = tanimotos\n",
    "        mcs_overlaps[i:end_idx] = overlaps\n",
    "\n",
    "    return mcs_sizes, mcs_tanimotos, mcs_overlaps\n",
    "\n",
    "# Compute Tanimoto similarities in parallel\n",
    "# fp_types = ['Morgan', 'FeatMorgan', 'AtomPair', 'RDKit', 'Torsion', 'Layered', 'MACCS']\n",
    "# for fp in fp_types:\n",
    "#     df[f\"Tanimoto_{fp}\"] = parallel_tanimoto(fp)\n",
    "\n",
    "# Compute MCS Features in parallel\n",
    "df[\"MCS_Size\"], df[\"MCS_Tanimoto\"], df[\"MCS_Overlap\"] = parallel_mcs()\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv(\"smiley_mcs_paper.csv\", index=False)\n",
    "print(\"✅ Feature extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiley_1</th>\n",
       "      <th>smiley_2</th>\n",
       "      <th>Tanimoto_Morgan</th>\n",
       "      <th>Tanimoto_FeatMorgan</th>\n",
       "      <th>Tanimoto_AtomPair</th>\n",
       "      <th>Tanimoto_RDKit</th>\n",
       "      <th>Tanimoto_Torsion</th>\n",
       "      <th>Tanimoto_Layered</th>\n",
       "      <th>Tanimoto_MACCS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...</td>\n",
       "      <td>CC1=CC(=CC=C1)NC2=C(C=NC=C2)S(=O)(=O)NC(=O)NC(C)C</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.172727</td>\n",
       "      <td>0.307380</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.307380</td>\n",
       "      <td>0.316327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...</td>\n",
       "      <td>C1=CC(=C2C(=C1NCCNCCO)C(=O)C3=C(C=CC(=C3C2=O)O...</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.166355</td>\n",
       "      <td>0.329656</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.329656</td>\n",
       "      <td>0.506329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...</td>\n",
       "      <td>CCOC(=O)C1=C2CN(C(=O)C3=C(N2C=N1)C=CC(=C3)F)C</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.554217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...</td>\n",
       "      <td>C1CCC(CC1)NC(=O)N(CCCl)N=O</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.108635</td>\n",
       "      <td>0.121653</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.121653</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...</td>\n",
       "      <td>C1=CC(=CC(=C1)C(F)(F)F)/C(=N\\OCCCCC(=O)O)/C2=C...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.243031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243031</td>\n",
       "      <td>0.337209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            smiley_1  \\\n",
       "0  C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...   \n",
       "1  C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...   \n",
       "2  C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...   \n",
       "3  C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...   \n",
       "4  C[S+](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H]([C...   \n",
       "\n",
       "                                            smiley_2  Tanimoto_Morgan  \\\n",
       "0  CC1=CC(=CC=C1)NC2=C(C=NC=C2)S(=O)(=O)NC(=O)NC(C)C         0.085106   \n",
       "1  C1=CC(=C2C(=C1NCCNCCO)C(=O)C3=C(C=CC(=C3C2=O)O...         0.061728   \n",
       "2      CCOC(=O)C1=C2CN(C(=O)C3=C(N2C=N1)C=CC(=C3)F)C         0.130435   \n",
       "3                         C1CCC(CC1)NC(=O)N(CCCl)N=O         0.062500   \n",
       "4  C1=CC(=CC(=C1)C(F)(F)F)/C(=N\\OCCCCC(=O)O)/C2=C...         0.085106   \n",
       "\n",
       "   Tanimoto_FeatMorgan  Tanimoto_AtomPair  Tanimoto_RDKit  Tanimoto_Torsion  \\\n",
       "0             0.083333           0.172727        0.307380          0.010870   \n",
       "1             0.060241           0.166355        0.329656          0.010753   \n",
       "2             0.115789           0.197150        0.433962          0.131868   \n",
       "3             0.060976           0.108635        0.121653          0.041667   \n",
       "4             0.082474           0.231293        0.243031          0.000000   \n",
       "\n",
       "   Tanimoto_Layered  Tanimoto_MACCS  \n",
       "0          0.307380        0.316327  \n",
       "1          0.329656        0.506329  \n",
       "2          0.433962        0.554217  \n",
       "3          0.121653        0.344828  \n",
       "4          0.243031        0.337209  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['smiley_1', 'smiley_2', 'Tanimoto_Morgan', 'Tanimoto_FeatMorgan',\n",
      "       'Tanimoto_AtomPair', 'Tanimoto_RDKit', 'Tanimoto_Torsion',\n",
      "       'Tanimoto_Layered', 'Tanimoto_MACCS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"tanimoto_smiley_paper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(\"paper_final_data.csv\")\n",
    "df2 = pd.read_csv(\"tanimoto_smiley_paper.csv\")\n",
    "\n",
    "# Merge on smiley1 and smiley2\n",
    "merged_df = pd.merge(df1, df2, on=[\"smiley_1\", \"smiley_2\"], how=\"inner\")  # Use \"outer\" for full join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"training_data_paper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('training_data_paper.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/miniconda3/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [1836 1837 1838 1839 1840 1841 1842 1843 1846 1847 1862 1863 1874 1875\n",
      " 1878 1879 1888 1889 1900 1901 1912 1913 1914 1915 1916 1917 1918 1919\n",
      " 1920 1921 1922 1923 1924 1925 1928 1929 1938 1939 1944 1945 1946 1947\n",
      " 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961\n",
      " 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1976 1977\n",
      " 1978 1979 1980 1981 1982 1983 1986 1987 1988 1989 1990 1991 1992 1993\n",
      " 1994 1995 1996 1997 1998 1999 2000 2001 2004 2005 2020 2021 2032 2033\n",
      " 2036 2037 2046 2047 2058 2059 2070 2071 2072 2073 2074 2075 2076 2077\n",
      " 2078 2079 2080 2081 2082 2083 2086 2087 2096 2097 2102 2103 2104 2105\n",
      " 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119\n",
      " 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2134 2135\n",
      " 2136 2137 2138 2139 2140 2141 2144 2145 2146 2147 2148 2149 2150 2151\n",
      " 2744 2745 2764 2765 2904 2905 2928 2929 2936 2937 2938 2939 2944 2945\n",
      " 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2960 2961 2962 2963\n",
      " 2968 2969 2970 2971 2972 2973 2974 2975 2976 2977 2978 2979 3000 3001\n",
      " 3024 3025 3032 3033 3034 3035 3036 3037 3040 3041 3046 3047 3054 3055\n",
      " 3056 3057 3058 3059 3062 3063 3068 3069 3076 3077 3078 3079 3080 3081\n",
      " 3082 3083 3084 3085 3090 3091 3098 3099 3100 3101 3102 3103 3104 3105\n",
      " 3106 3107 3112 3113 3120 3121 3122 3123 3124 3125 3128 3129 3134 3135\n",
      " 3136 3137 3142 3143 3144 3145 3146 3147 3150 3151 3156 3157 3158 3159] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/prateek/miniconda3/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/home/prateek/miniconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [05:01:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m fs_methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutual Information\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRFE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLasso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelectKBest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning feature selection...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_feature_selection\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfs_methods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(results)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# ========== 8. Train Models ==========\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ========== 1. Stratified Sampling ==========\n",
    "df_sample = df\n",
    "\n",
    "# ========== 2. Extract paired features ==========\n",
    "feature_pairs = {}\n",
    "for col in df_sample.columns:\n",
    "    if \"_\" in col and col not in [\"smiley_1\", \"smiley_2\", \"drug_1\", \"drug_2\", \"matched\", \"common_targets\"]:\n",
    "        base = \"_\".join(col.split(\"_\")[:-1])\n",
    "        feature_pairs.setdefault(base, []).append(col)\n",
    "\n",
    "paired_features = [pair for pair in feature_pairs.values() if len(pair) == 2]\n",
    "flattened_features = [f for pair in paired_features for f in pair]\n",
    "\n",
    "additional_features = [\n",
    "    \"Tanimoto_Morgan\", \"Tanimoto_FeatMorgan\", \"Tanimoto_AtomPair\",\n",
    "    \"Tanimoto_RDKit\", \"Tanimoto_Torsion\", \"Tanimoto_Layered\", \"Tanimoto_MACCS\"\n",
    "]\n",
    "flattened_features.extend(additional_features)\n",
    "\n",
    "# ========== 3. Prepare X and y ==========\n",
    "X = df_sample[flattened_features]\n",
    "y = df_sample[\"matched\"].values\n",
    "\n",
    "# ========== 4. Scale ==========\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ========== 5. Train-test split ==========\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ========== 6. Feature Pair Enforcer ==========\n",
    "def enforce_feature_pairs(selected_features):\n",
    "    selected_pairs = []\n",
    "    for pair in paired_features:\n",
    "        if any(f in selected_features for f in pair):\n",
    "            selected_pairs.extend(pair)\n",
    "    return list(set(selected_pairs + [f for f in selected_features if f in additional_features]))\n",
    "\n",
    "# ========== 7. Feature Selection Methods ==========\n",
    "def run_feature_selection(method):\n",
    "    try:\n",
    "        if method == \"Mutual Information\":\n",
    "            scores = mutual_info_classif(X_train, y_train)\n",
    "            selected = sorted(zip(flattened_features, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "            return method, enforce_feature_pairs([f for f, _ in selected])\n",
    "\n",
    "        elif method == \"RFE\":\n",
    "            rfe = RFE(LogisticRegression(max_iter=1000), n_features_to_select=20)\n",
    "            rfe.fit(X_train, y_train)\n",
    "            return method, enforce_feature_pairs([f for f, s in zip(flattened_features, rfe.support_) if s])\n",
    "\n",
    "        elif method == \"Random Forest\":\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train, y_train)\n",
    "            scores = rf.feature_importances_\n",
    "            selected = sorted(zip(flattened_features, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "            return method, enforce_feature_pairs([f for f, _ in selected])\n",
    "\n",
    "        elif method == \"Lasso\":\n",
    "            lasso = Lasso(alpha=0.01)\n",
    "            lasso.fit(X_train, y_train)\n",
    "            return method, enforce_feature_pairs([f for f, c in zip(flattened_features, lasso.coef_) if abs(c) > 0])\n",
    "\n",
    "        elif method == \"XGBoost\":\n",
    "            xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "            xgb.fit(X_train, y_train)\n",
    "            scores = xgb.feature_importances_\n",
    "            selected = sorted(zip(flattened_features, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "            return method, enforce_feature_pairs([f for f, _ in selected])\n",
    "\n",
    "        elif method == \"SelectKBest\":\n",
    "            skb = SelectKBest(score_func=f_classif, k=20).fit(X_train, y_train)\n",
    "            return method, enforce_feature_pairs([f for f, s in zip(flattened_features, skb.get_support()) if s])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {method}: {e}\")\n",
    "        return method, []\n",
    "\n",
    "fs_methods = [\"Mutual Information\", \"RFE\", \"Random Forest\", \"Lasso\", \"XGBoost\", \"SelectKBest\"]\n",
    "\n",
    "print(\"Running feature selection...\")\n",
    "results = Parallel(n_jobs=-1)(delayed(run_feature_selection)(method) for method in fs_methods)\n",
    "selected_features = dict(results)\n",
    "\n",
    "# ========== 8. Train Models ==========\n",
    "def train_model(X_train, X_test, y_train, y_test, features, model):\n",
    "    idx = [flattened_features.index(f) for f in features if f in flattened_features]\n",
    "    X_train_sel = X_train[:, idx]\n",
    "    X_test_sel = X_test[:, idx]\n",
    "\n",
    "    model.fit(X_train_sel, y_train)\n",
    "    y_pred = model.predict(X_test_sel)\n",
    "    y_proba = model.predict_proba(X_test_sel)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "    return accuracy_score(y_test, y_pred), roc_auc_score(y_test, y_proba)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "}\n",
    "\n",
    "accuracy_results, auc_results = {}, {}\n",
    "\n",
    "print(\"Training models...\")\n",
    "for model_name, model in tqdm(models.items()):\n",
    "    accuracy_results[model_name] = {}\n",
    "    auc_results[model_name] = {}\n",
    "    for method, features in selected_features.items():\n",
    "        acc, auc = train_model(X_train, X_test, y_train, y_test, features, model)\n",
    "        accuracy_results[model_name][method] = acc\n",
    "        auc_results[model_name][method] = auc\n",
    "\n",
    "# ========== 9. Print Results ==========\n",
    "print(\"\\nFeature Selection Summary:\")\n",
    "for method, features in selected_features.items():\n",
    "    print(f\"{method}: {features}\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "for model in models:\n",
    "    print(f\"\\n{model}:\")\n",
    "    for method in fs_methods:\n",
    "        print(f\"  {method}: Accuracy = {accuracy_results[model][method]:.4f}, AUC = {auc_results[model][method]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
